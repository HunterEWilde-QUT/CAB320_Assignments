{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea6071f-8e0f-4cfb-ae67-323337c91ba2",
   "metadata": {},
   "source": [
    "# CAB320 Assignment 2 - Transfer Learning\n",
    "Anthony Vanderkop, Thierry Peynot, Frederic Maire (Jupyter Notebook template: 2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69e9bb-1cf5-49cd-9cc5-8680461ea876",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "The functions and classes defined in this module will be called by the marker without modification. \n",
    "You should complete the functions and classes according to their specified interfaces.\n",
    "\n",
    "No partial marks will be awarded for functions that do not meet the specifications of the interfaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a200b943-94f5-4099-a330-1e2b89aaff27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:23.157725Z",
     "start_time": "2025-05-25T06:43:19.728618Z"
    }
   },
   "source": [
    "### LIBRARY IMPORTS ###\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.applications as ka\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2708ef2-ff27-4db9-8563-da2c648d69bd",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Implement the my_team()function "
   ]
  },
  {
   "cell_type": "code",
   "id": "34388e34-bd90-4e02-9759-f7c4bd9a598e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:23.188650Z",
     "start_time": "2025-05-25T06:43:23.175307Z"
    }
   },
   "source": [
    "def my_team():\n",
    "    \"\"\"\n",
    "    Return the list of the team members of this assignment submission as a list\n",
    "    of triplet of the form (student_number, first_name, last_name)\n",
    "\n",
    "    \"\"\"\n",
    "    return [ (11032553, 'Hunter', 'Wilde'), (12026395, 'Oliver', 'Kele') ]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a005199c-572f-4968-9438-881ad28b62b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:23.774358Z",
     "start_time": "2025-05-25T06:43:23.761175Z"
    }
   },
   "source": [
    "my_team()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11032553, 'Hunter', 'Wilde'), (12026395, 'Oliver', 'Kele')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f05d766-c455-44f0-a0db-ae271e5926cf",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Download the small_flower_dataset from Canvas and load the data"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:23.851663Z",
     "start_time": "2025-05-25T06:43:23.838342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Global variable to store class name to index mapping\n",
    "class_to_idx = {}\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load in the dataset from its home path. Path should be a string of the path\n",
    "    to the home directory the dataset is found in. Should return numpy arrays\n",
    "    with paired images and class labels.\n",
    "\n",
    "    This function:\n",
    "    1. Loads images from the small_flower_dataset directory structure\n",
    "        - The dataset is organized with class folders, where folder name = class name\n",
    "    2. Organizes them into features (X) and labels (Y)\n",
    "    3. Returns tuple of (X, Y) where:\n",
    "       - X is a numpy array of images with shape (n_samples, height, width, channels)\n",
    "       - Y is a numpy array of integer labels with shape (n_samples,)\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Get the global variable class_to_idx\n",
    "    global class_to_idx\n",
    "\n",
    "    # Get all subdirectories (class folders) in the path\n",
    "    class_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "\n",
    "    # Clear the existing dictionary and assign new integer indices to each class\n",
    "    class_to_idx.clear()\n",
    "    for idx, class_name in enumerate(sorted(class_dirs)):\n",
    "        class_to_idx[class_name] = idx\n",
    "\n",
    "    # Set target size for MobileNetV2 and to account for different image sizes in dataset\n",
    "    target_size = (224, 224)\n",
    "\n",
    "    # Process each class directory\n",
    "    for class_name in class_dirs:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        class_idx = class_to_idx[class_name]\n",
    "\n",
    "        # Get all image files in the class directory\n",
    "        image_files = os.listdir(class_path)\n",
    "\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "            # Load image and convert to array with target size of 224x224 (MobileNetV2 input size)\n",
    "            img = keras.utils.load_img(img_path, target_size=target_size)\n",
    "            img_array = keras.utils.img_to_array(img)\n",
    "\n",
    "            # Add to dataset\n",
    "            X.append(img_array)\n",
    "            Y.append(class_idx)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Printf statements to check the dataset has been loaded correctly \n",
    "    print(f\"Dataset loaded: {X.shape[0]} images, {len(class_to_idx)} classes\")\n",
    "    print(f\"Image shape: {X.shape[1:]}\")\n",
    "    print(f\"Classes: {class_to_idx}\")\n",
    "\n",
    "    return X, Y"
   ],
   "id": "82076152cfc036dc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:25.717356Z",
     "start_time": "2025-05-25T06:43:23.884228Z"
    }
   },
   "cell_type": "code",
   "source": "X, Y = load_data(\"./small_flower_dataset\")",
   "id": "6d655e0a54ffe32e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1000 images, 5 classes\n",
      "Image shape: (224, 224, 3)\n",
      "Classes: {'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3\n",
    "Prepare your training, validation and test sets for the non-accelerated version of transfer learning."
   ],
   "id": "1e56394ba276d68d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:25.764260Z",
     "start_time": "2025-05-25T06:43:25.750223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data(X, Y, train_fraction, randomize=False, eval_set=True):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets. If eval_set is True, also create\n",
    "    an evaluation dataset. There should be two outputs if eval_set is False, or\n",
    "    three outputs (train, test, eval) if eval_set is True.\n",
    "\n",
    "    This function performs stratified splitting to maintain class balance, ensuring\n",
    "    each split contains the same proportion of samples from each class.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.array\n",
    "        Array of image data with shape (n_samples, height, width, channels)\n",
    "    Y : numpy.array\n",
    "        Array of class labels with shape (n_samples)\n",
    "    train_fraction : float\n",
    "        Fraction of data to use for training (between 0 and 1)\n",
    "    randomize : bool, optional\n",
    "        Whether to randomize the data before splitting (default: False)\n",
    "    eval_set : bool, optional\n",
    "        Whether to create a separate evaluation/validation set (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If eval_set=True:\n",
    "        (train_set, eval_set, test_set) : tuple of tuples\n",
    "            Each inner tuple contains (images, labels) for the respective set\n",
    "    If eval_set=False:\n",
    "        (train_set, test_set) : tuple of tuples\n",
    "            Each inner tuple contains (images, labels) for the respective set\n",
    "    \"\"\"\n",
    "    # Get classes (known to be unique, but uses .unique() to catch any bugs/errors from earlier functions)\n",
    "    unique_classes = np.unique(Y)\n",
    "\n",
    "    # Lists to store indices for each split\n",
    "    train_indices = []\n",
    "    eval_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # For each class, split its indices to maintain class balance\n",
    "    for class_idx in unique_classes:\n",
    "        # Get indices of samples belonging to this class\n",
    "        class_indices = np.flatnonzero(Y == class_idx)\n",
    "        num_samples = len(class_indices)\n",
    "\n",
    "        # Randomize if requested\n",
    "        if randomize:\n",
    "            np.random.shuffle(class_indices)\n",
    "\n",
    "        # Calculate split points\n",
    "        train_end = int(num_samples * train_fraction)\n",
    "\n",
    "        if eval_set:\n",
    "            # Ensure equal sizes for validation and test sets\n",
    "            remaining = num_samples - train_end\n",
    "            val_samples = remaining // 2\n",
    "\n",
    "            eval_end = train_end + val_samples\n",
    "\n",
    "            # Add indices to respective sets\n",
    "            train_indices.extend(class_indices[:train_end].tolist())\n",
    "            eval_indices.extend(class_indices[train_end:eval_end].tolist())\n",
    "            test_indices.extend(class_indices[eval_end:].tolist())\n",
    "        else:\n",
    "            # No eval set, just train and test\n",
    "            train_indices.extend(class_indices[:train_end].tolist())\n",
    "            test_indices.extend(class_indices[train_end:].tolist())\n",
    "\n",
    "    # Convert lists to integer numpy arrays explicitly\n",
    "    train_indices = np.array(train_indices, dtype=int)\n",
    "    test_indices = np.array(test_indices, dtype=int)\n",
    "\n",
    "    # Create the final datasets\n",
    "    train_set = (X[train_indices], Y[train_indices])\n",
    "    test_set = (X[test_indices], Y[test_indices])\n",
    "\n",
    "    if eval_set:\n",
    "        eval_indices = np.array(eval_indices, dtype=int)\n",
    "        eval_set_data = (X[eval_indices], Y[eval_indices])\n",
    "\n",
    "        # Print statement for debugging/check\n",
    "        print(f\"Split complete: {len(train_indices)} train, {len(eval_indices)} validation, {len(test_indices)} test images\")\n",
    "        return train_set, eval_set_data, test_set\n",
    "    else:\n",
    "        # Print statement for debugging/check\n",
    "        print(f\"Split complete: {len(train_indices)} train, {len(test_indices)} test images\")\n",
    "        return train_set, test_set"
   ],
   "id": "cbd853a0b0d3d0b6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:26.039912Z",
     "start_time": "2025-05-25T06:43:25.827338Z"
    }
   },
   "cell_type": "code",
   "source": "train_set, eval_set, test_set = split_data(X, Y, train_fraction = 0.8)",
   "id": "82c93bac5b458a90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete: 800 train, 100 validation, 100 test images\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Report: Include details of how you have split the data to perform this training. Ensure the split is reasonable and does not introduce class imbalance during training",
   "id": "81eff9d2469b0706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The array of class labels `Y` is first passed through a function which extracts only unique entries to ensure there are no repeated classes; the filtered class array `unique_classes` is then iterated over as each entry in `Y` is assigned to a unique class, resulting in a local instance of `class_indices` for each unique class, which contains all entries of that class (should be 1, excepting anomalies). The set of classes is split between the arrays `train_set` & `test_set` according to the ratio assigned to `train_fraction`.",
   "id": "130f8c6b5abc4272"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 4\n",
    "Using the tf.keras.applications module download a pretrained MobileNetV2 network. "
   ],
   "id": "5fb9287caecb3579"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:26.178886Z",
     "start_time": "2025-05-25T06:43:26.166545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load in a pretrained MobileNetV2 model using the keras.applications module.\n",
    "\n",
    "    This function:\n",
    "    1. Downloads a pretrained MobileNetV2 network with weights from ImageNet\n",
    "    2. Sets up the model with the input shape appropriate for our dataset (224, 224, 3)\n",
    "    3. Ensures the base model layers are not trainable (frozen) for transfer learning\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        A pretrained MobileNetV2 model with frozen base layers, ready for transfer learning\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pretrained model without the top classification layer\n",
    "    # Include weights from ImageNet, and use input shape of 224x224x3\n",
    "    base_model = ka.MobileNetV2(weights='imagenet', \n",
    "                            include_top=False, \n",
    "                            input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the layers in the base model so they won't be trained\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Print f statements to ensure model loaded correctly\n",
    "    print(f\"MobileNetV2 model loaded with {len(base_model.layers)} layers\")\n",
    "    print(f\"Input shape: {base_model.input_shape}\")\n",
    "    print(f\"Output shape: {base_model.output_shape}\")\n",
    "\n",
    "    return base_model"
   ],
   "id": "2f0fa379625b6b7e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:31.027263Z",
     "start_time": "2025-05-25T06:43:26.274138Z"
    }
   },
   "cell_type": "code",
   "source": "model = load_model()",
   "id": "735dd2ea9bc5f744",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001B[1m9406464/9406464\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 0us/step\n",
      "MobileNetV2 model loaded with 154 layers\n",
      "Input shape: (None, 224, 224, 3)\n",
      "Output shape: (None, 7, 7, 1280)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 5\n",
    "Replace the last layer of the downloaded neural network with a Dense layer of the appropriate shape for the 5 classes of the small flower dataset {(x1,t1), (x2,t2),..., (xm,tm)}.\n",
    "\n",
    "## Task 6\n",
    "Compile and train your model with an SGD optimizer using the following parameters learning_rate=0.01, momentum=0.0, nesterov=False. (NB: The SGD class description can be found at https://keras.io/api/optimizers/sgd/  )"
   ],
   "id": "1c99f0eab0f10e65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:43:31.074069Z",
     "start_time": "2025-05-25T06:43:31.059923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transfer_learning(train_set, eval_set, model, parameters=(0.01, 0.0, False)):\n",
    "    \"\"\"\n",
    "    Implement and perform standard transfer learning here.\n",
    "\n",
    "    Inputs:\n",
    "        - train_set: list or tuple of the training images and labels in the\n",
    "            form (images, labels) for training the classifier\n",
    "        - eval_set: list or tuple of the images and labels used in evaluating\n",
    "            the model during training, in the form (images, labels)\n",
    "        - model: an instance of tf.keras.applications.MobileNetV2\n",
    "        - parameters: tuple of parameters to use during training:\n",
    "            (learning_rate, momentum, nesterov)\n",
    "\n",
    "    Outputs:\n",
    "        - model : an instance of tf.keras.applications.MobileNetV2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack parameters\n",
    "    learning_rate, momentum, nesterov = parameters\n",
    "\n",
    "    # Unpack datasets\n",
    "    x_train, y_train = train_set\n",
    "    x_eval, y_eval = eval_set\n",
    "\n",
    "    # Get number of classes from the dataset\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    # Add a global average pooling layer and a new classification layer\n",
    "    x = model.output\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    predictions = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the final model\n",
    "    final_model = keras.models.Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model with specified parameters\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "    final_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',  # Use for integer class labels\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Define callbacks for training\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\",\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = final_model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_eval, y_eval),\n",
    "        epochs=50,  # Can be adjusted\n",
    "        batch_size=32,  # Can be adjusted\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Store the training history as an attribute for later analysis\n",
    "    final_model.history = history\n",
    "\n",
    "    return final_model"
   ],
   "id": "3495fbabef5617b2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-25T06:43:31.136703Z"
    }
   },
   "cell_type": "code",
   "source": "model = transfer_learning(train_set, eval_set, model)",
   "id": "e24a2b4a1de427cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 304ms/step - accuracy: 0.2014 - loss: 2.0974\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24000, saving model to best_model.keras\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 409ms/step - accuracy: 0.2019 - loss: 2.0889 - val_accuracy: 0.2400 - val_loss: 1.6733\n",
      "Epoch 2/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 321ms/step - accuracy: 0.3008 - loss: 1.6308\n",
      "Epoch 2: val_accuracy improved from 0.24000 to 0.31000, saving model to best_model.keras\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 374ms/step - accuracy: 0.3010 - loss: 1.6293 - val_accuracy: 0.3100 - val_loss: 1.6763\n",
      "Epoch 3/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 299ms/step - accuracy: 0.3489 - loss: 1.5208\n",
      "Epoch 3: val_accuracy improved from 0.31000 to 0.41000, saving model to best_model.keras\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 355ms/step - accuracy: 0.3497 - loss: 1.5199 - val_accuracy: 0.4100 - val_loss: 1.4683\n",
      "Epoch 4/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 306ms/step - accuracy: 0.3846 - loss: 1.4393\n",
      "Epoch 4: val_accuracy improved from 0.41000 to 0.43000, saving model to best_model.keras\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 364ms/step - accuracy: 0.3855 - loss: 1.4374 - val_accuracy: 0.4300 - val_loss: 1.4266\n",
      "Epoch 5/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 313ms/step - accuracy: 0.4175 - loss: 1.3871\n",
      "Epoch 5: val_accuracy did not improve from 0.43000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 355ms/step - accuracy: 0.4181 - loss: 1.3859 - val_accuracy: 0.3800 - val_loss: 1.5055\n",
      "Epoch 6/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 297ms/step - accuracy: 0.5132 - loss: 1.2630\n",
      "Epoch 6: val_accuracy did not improve from 0.43000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 341ms/step - accuracy: 0.5127 - loss: 1.2641 - val_accuracy: 0.3400 - val_loss: 1.4771\n",
      "Epoch 7/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 305ms/step - accuracy: 0.4931 - loss: 1.2235\n",
      "Epoch 7: val_accuracy improved from 0.43000 to 0.50000, saving model to best_model.keras\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 358ms/step - accuracy: 0.4935 - loss: 1.2236 - val_accuracy: 0.5000 - val_loss: 1.3200\n",
      "Epoch 8/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 323ms/step - accuracy: 0.5257 - loss: 1.1623\n",
      "Epoch 8: val_accuracy did not improve from 0.50000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 365ms/step - accuracy: 0.5253 - loss: 1.1634 - val_accuracy: 0.4500 - val_loss: 1.4738\n",
      "Epoch 9/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 304ms/step - accuracy: 0.5531 - loss: 1.1335\n",
      "Epoch 9: val_accuracy did not improve from 0.50000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 344ms/step - accuracy: 0.5518 - loss: 1.1351 - val_accuracy: 0.4000 - val_loss: 1.4796\n",
      "Epoch 10/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 296ms/step - accuracy: 0.5243 - loss: 1.1426\n",
      "Epoch 10: val_accuracy did not improve from 0.50000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 339ms/step - accuracy: 0.5243 - loss: 1.1437 - val_accuracy: 0.5000 - val_loss: 1.3125\n",
      "Epoch 11/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 310ms/step - accuracy: 0.5400 - loss: 1.1138\n",
      "Epoch 11: val_accuracy did not improve from 0.50000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 352ms/step - accuracy: 0.5408 - loss: 1.1132 - val_accuracy: 0.4500 - val_loss: 1.3430\n",
      "Epoch 12/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 301ms/step - accuracy: 0.5903 - loss: 1.0930\n",
      "Epoch 12: val_accuracy did not improve from 0.50000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 346ms/step - accuracy: 0.5904 - loss: 1.0925 - val_accuracy: 0.4900 - val_loss: 1.2726\n",
      "Epoch 13/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 304ms/step - accuracy: 0.5562 - loss: 1.0722\n",
      "Epoch 13: val_accuracy did not improve from 0.50000\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 344ms/step - accuracy: 0.5568 - loss: 1.0721 - val_accuracy: 0.4100 - val_loss: 1.3176\n",
      "Epoch 14/50\n",
      "\u001B[1m13/25\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m3s\u001B[0m 298ms/step - accuracy: 0.5536 - loss: 1.1034"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Report: Provide an overview of the function and how it works.",
   "id": "ed618238f139d0f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After first unpacking all necessary data from the input parameters, two new layers (a GAP layer & a softmax classification layer) are added to the model, which are derived from the output of the model's previous version. The new model is compiled, reusing the input parameters given for the previous model. Checkpoints & early stopping points are defined for time-saving during training. Finally, the previous version of the model is appended to the model history.",
   "id": "5b5d16b37e0fa58a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 7\n",
    "Plot the training and validation errors and accuracies of standard transfer "
   ],
   "id": "3c90419b8a88c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T03:33:52.016286900Z",
     "start_time": "2025-05-20T03:54:40.261779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_errors(model):\n",
    "    \"\"\"\n",
    "    Plot the training & validation errors & accuracies from the model's training history.\n",
    "\n",
    "    This function:\n",
    "    1. Creates two subplots: one for error & one for accuracy.\n",
    "    2. Plots both training & validation metrics on each subplot.\n",
    "    3. Adds appropriate labels, titles, & legend.\n",
    "    \"\"\"\n",
    "    # Get the number of epochs\n",
    "    epochs = range(1, len(model.history['loss']) + 1)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot training & validation error\n",
    "    ax1.plot(epochs, model.history['loss'], 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, model.history['val_loss'], 'r-', label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot training & validation accuracies\n",
    "    ax2.plot(epochs, model.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "    ax2.plot(epochs, model.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "3f7bf63d74eb8ab9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T03:33:52.017794700Z",
     "start_time": "2025-05-20T03:55:11.248426Z"
    }
   },
   "cell_type": "code",
   "source": "plot_errors(model)",
   "id": "aa76dccddd01d0b8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Call the plot_errors function to visualize training and validation metrics\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mplot_errors\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 12\u001B[0m, in \u001B[0;36mplot_errors\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mPlot the training and validation errors (loss) and accuracies from the model's training history.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03m4. Adds appropriate labels, titles, and legend\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Get the number of epochs\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Create a figure with two subplots\u001B[39;00m\n\u001B[0;32m     15\u001B[0m fig, (ax1, ax2) \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 8\n",
    "Experiment with 3 different orders of magnitude for the learning rate. Plot the results and discuss in the below markdown cell"
   ],
   "id": "9aa6f483a451d5d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_slowest = transfer_learning(train_set, eval_set, model, (0.001, 0.0, False)) # 0.1x the base learning rate\n",
    "model_faster = transfer_learning(train_set, eval_set, model, (0.1, 0.0, False)) # 10x the base learning rate\n",
    "model_fastest = transfer_learning(train_set, eval_set, model, (1, 0.0, False)) # 100x the base learning rate\n",
    "\n",
    "# Plotting the results\n",
    "plot_errors(model_slowest)\n",
    "plot_errors(model_faster)\n",
    "plot_errors(model_fastest)"
   ],
   "id": "d6961fc5190ae971"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task 8 Analysis and discussion\n",
   "id": "c237ebbde24d4f0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "721ff5d2cb40a565"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 9\n",
    "Run the resulting classifier on your test dataset using results from the best learning rate you experimented with. Compute and display the confusion matrix. "
   ],
   "id": "2c1bd24e5cb52a47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## Your code",
   "id": "f164967c952860a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 10\n",
    "Compute the precision, recall, and f1 scores of your classifier on the test dataset using the best learning rate. Report on the results and comment. "
   ],
   "id": "c1a7e4f98ac0c5da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## Your code",
   "id": "f90e20a483277e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 11\n",
    "Perform k-fold validation on the dataset with k = 3. "
   ],
   "id": "5d33ab6385b3dd3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def k_fold_validation(features, ground_truth, classifier, k=2):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - features: np.ndarray of features in the dataset\n",
    "        - ground_truth: np.ndarray of class values associated with the features\n",
    "        - fit_func: f\n",
    "        - classifier: class object with both fit() and predict() methods which\n",
    "        can be applied to subsets of the features and ground_truth inputs.\n",
    "        - predict_func: function, calling predict_func(features) should return\n",
    "        a numpy array of class predictions which can in turn be input to the\n",
    "        functions in this script to calculate performance metrics.\n",
    "        - k: int, number of sub-sets to partition the data into. default is k=2\n",
    "    Outputs:\n",
    "        - avg_metrics: np.ndarray of shape (3, c) where c is the number of classes.\n",
    "        The first row is the average precision for each class over the k\n",
    "        validation steps. Second row is recall and third row is f1 score.\n",
    "        - sigma_metrics: np.ndarray, each value is the standard deviation of\n",
    "        the performance metrics [precision, recall, f1_score]\n",
    "    \"\"\"\n",
    "\n",
    "    #split data\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    #go through each partition and use it as a test set.\n",
    "    for partition_no in range(k):\n",
    "        #determine test and train sets\n",
    "        ### YOUR CODE HERE###\n",
    "\n",
    "        #fit model to training data and perform predictions on the test set\n",
    "        classifier.fit(train_features, train_classes)\n",
    "        predictions = classifier.predict(test_features)\n",
    "\n",
    "        #calculate performance metrics\n",
    "        ### YOUR CODE HERE###\n",
    "\n",
    "    #perform statistical analyses on metrics\n",
    "    ### YOUR CODE HERE###\n",
    "\n",
    "    raise NotImplementedError\n",
    "    return avg_metrics, sigma_metrics"
   ],
   "id": "89965176c910f515"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8b614-53db-4adf-ae35-e96b0b0fa485",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code\n",
    "# xx = k_fold_validation(xx, xx, xx, xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201857c-0cd2-487b-a2ad-4e6abe619fbf",
   "metadata": {},
   "source": [
    "Comment on the results and any differences with the previous test-train split. \n",
    "Repeat with two different values for k and comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16615b4-bbdd-445e-9835-34df03f3c8cb",
   "metadata": {},
   "source": [
    "### Comments and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bae8a-30cc-4143-8de0-80477170caf6",
   "metadata": {},
   "source": [
    "## Task 12\n",
    "With the best learning rate that you found in the previous task, add a non-zero momentum to the training with the SGD optimizer (consider 3 values for the momentum). Report on how your results change.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44e49f-4adc-4d63-8bb3-15e39bb29e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5fa9c-94a3-4c7a-91a1-8d6afe9c4dce",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571260a-66e9-4b44-b012-74d7eee13359",
   "metadata": {},
   "source": [
    "## Task 13\n",
    "Now using “accelerated transfer learning”, repeat the training process (k-fold validation is optional this time). You should prepare your training, validation and test sets based on {(F(x1).t1), (F(x2),t2),...,(F(xm),tm)}, and re-do Task 12. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefd040-d00d-4869-8c96-67b5934d05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accelerated_learning(train_set, eval_set, model, parameters):\n",
    "    \"\"\"\n",
    "    Implement and perform accelerated transfer learning here.\n",
    "\n",
    "    Inputs:\n",
    "        - train_set: list or tuple of the training images and labels in the\n",
    "            form (images, labels) for training the classifier\n",
    "        - eval_set: list or tuple of the images and labels used in evaluating\n",
    "            the model during training, in the form (images, labels)\n",
    "        - model: an instance of tf.keras.applications.MobileNetV2\n",
    "        - parameters: list or tuple of parameters to use during training:\n",
    "            (learning_rate, momentum, nesterov)\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "        - model : an instance of tf.keras.applications.MobileNetV2\n",
    "\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338442d-a821-40c1-9b65-2fed7e552da2",
   "metadata": {},
   "source": [
    "\n",
    "Plot and comment on the results and differences against the standard implementation of transfer learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0b0f6-99fc-4c28-bc2e-b8e0954279b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25897cb9-3a50-448a-98a0-d75b7cebe769",
   "metadata": {},
   "source": [
    "### Your Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39cbe5-1894-4083-b8be-81b00b8822a4",
   "metadata": {},
   "source": [
    "## Task 14\n",
    "Use the results of all experiments to make suggestions for future work and recommendations for parameter values to anyone else who may be interested in a similar implementation of transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d30537-a23b-4678-9336-a08a5c61e14f",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218070e-f721-44d4-9efa-6b7004af906a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
